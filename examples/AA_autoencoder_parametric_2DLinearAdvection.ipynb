{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d04dff5",
   "metadata": {},
   "source": [
    "## Notebook to train an Advection-Aware AutoEncoder for a parametric 2D Linear Advection example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96376f86",
   "metadata": {},
   "source": [
    "Consider the advection of a circular Gaussian pulse traveling in the positive $y-$direction through a rectangular domain, $\\Omega = [-100,100] \\times [0, 500]$ at a constant speed, $c$. The analytical solution is given by\n",
    "\\begin{align}\\label{eq:Gaussian_pulse}\n",
    "    u(x,y) = \\exp{ \\bigg\\{ -\\left(\\frac{(x -x_0)^2}{2 \\sigma_x^2} + \\frac{(y - y_0 - ct)^2}{2 \\sigma_y^2} \\right) \\bigg\\} },\n",
    "\\end{align}\n",
    "where $(x_0, y_0)$ is the initial location of the center of the pulse, $\\sigma_x$ and $\\sigma_y$ define the support of the pulse in the $x$ and $y$ directions, respectively. The domain is uniformly discretized into $100701$ computational nodes using $\\Delta x = 1$ and $\\Delta y = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02c91c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:24.652199Z",
     "start_time": "2022-02-15T21:47:22.083747Z"
    },
    "code_folding": [
     51
    ]
   },
   "outputs": [],
   "source": [
    "## Load modules\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import importlib\n",
    "from importlib import reload as reload\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow v\"+str(tf.__version__))\n",
    "if tf.__version__ == '1.15.0':\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "elif tf.__version__.split('.')[0] == 2: # in ['2.2.0','2.3.0']:\n",
    "    print(\"Setting Keras backend datatype\")\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, LSTM, InputLayer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "# tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, ScalarFormatter, FormatStrFormatter\n",
    "\n",
    "from matplotlib import animation\n",
    "matplotlib.rc('animation', html='html5')\n",
    "from IPython.display import display\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "\n",
    "\n",
    "# Plot parameters\n",
    "plt.rc('font', family='serif')\n",
    "plt.rcParams.update({'font.size': 20,\n",
    "                     'lines.linewidth': 2,\n",
    "                     'lines.markersize':10,\n",
    "                     'axes.labelsize': 16, # fontsize for x and y labels (was 10)\n",
    "                     'axes.titlesize': 20,\n",
    "                     'xtick.labelsize': 16,\n",
    "                     'ytick.labelsize': 16,\n",
    "                     'legend.fontsize': 16,\n",
    "                     'axes.linewidth': 2})\n",
    "\n",
    "import itertools\n",
    "colors = itertools.cycle(['r','g','b','m','y','c'])\n",
    "markers = itertools.cycle(['p','d','o','^','s','x','D','H','v','*'])\n",
    "\n",
    "try:\n",
    "    os.listdir(base_dir)\n",
    "except:\n",
    "    base_dir = os.getcwd()\n",
    "utils_dir  = os.path.join(base_dir,'../src/utils')\n",
    "nn_dir  = os.path.join(base_dir,'../src/nn_model')\n",
    "work_dir = os.path.join(base_dir,'../examples')\n",
    "# data_dir = '/gpfs/cwfs/sdutta/hfm_data/Pulse'\n",
    "data_dir = os.path.join(base_dir,'../data/')\n",
    "model_dir = os.path.join(base_dir,'../data/saved_models/')\n",
    "fig_dir  = os.path.join(base_dir,'../figures/')\n",
    "\n",
    "\n",
    "os.chdir(utils_dir)\n",
    "import data_utils as du\n",
    "import tf_utils as tu\n",
    "import plot_utils as pu\n",
    "\n",
    "os.chdir(nn_dir)\n",
    "import aa_autoencoder as aa\n",
    "os.chdir(work_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3036f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:24.656261Z",
     "start_time": "2022-02-15T21:47:24.653605Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'gpu:0' # select gpu:# or cpu:#\n",
    "epochs_u = 6000\n",
    "\n",
    "## Uncomment the following when training for variable pulse size\n",
    "# param_list = [ 5, 10, 20]; flag = 'sigma';  # pulse geometry as a parameter\n",
    "\n",
    "## Uncomment the following when training for variable pulse speed\n",
    "param_list = [1, 2, 3, 4]; flag = 'speed';    # pulse speed as a parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc353ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:27.286951Z",
     "start_time": "2022-02-15T21:47:24.658316Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Load snapshot data\n",
    "model ='Pulse'\n",
    "soln_names = ['pulse']\n",
    "\n",
    "snap_data = {}; times_offline = {}\n",
    "c = {}; sigma = {}\n",
    "snap_start = {}; snap_end = {}\n",
    "\n",
    "for indx,val in enumerate(param_list):\n",
    "    if flag == 'sigma':\n",
    "        datafile = 'Gaussian2d_pulse_500x200_c1.00_sigma'+'%.4f'%val+'.npz'\n",
    "    elif flag == 'speed':\n",
    "        datafile = 'Gaussian2d_pulse_500x200_c'+'%.2f'%val+'_sigma5.0000.npz'\n",
    "    data = np.load(os.path.join(data_dir, datafile))\n",
    "    \n",
    "    c[indx] = float(datafile.split('_c')[1].split('_')[0])\n",
    "    sigma[indx] = float(datafile.split('_sigma')[1].split('.npz')[0])\n",
    "    print(\"\\n%d: Loading snapshots for c = %f, sigma = %f\"%(indx, c[indx], sigma[indx]))\n",
    "    \n",
    "    snap_data[indx], times_offline[indx], nodes, Nx, Ny, snap_start[indx], snap_end[indx] = du.read_data(data, soln_names)\n",
    "    \n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f077cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:30.178965Z",
     "start_time": "2022-02-15T21:47:27.288388Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Augment input snapshots for higher speed pulse systems\n",
    "\n",
    "ctr = len(snap_data.keys())\n",
    "\n",
    "for param in param_list:\n",
    "    if param == 2:\n",
    "        for indx,val in enumerate([41,]):\n",
    "            datafile = 'gaussian2d_pulse_500x200_c%.2f_sigma5.0000_y0%d.npz'%(param,val)\n",
    "            data = np.load(os.path.join(data_dir, datafile))\n",
    "\n",
    "            c[ctr] = float(datafile.split('_c')[1].split('_')[0])\n",
    "            sigma[ctr] = float(datafile.split('_sigma')[1].split('_y0')[0])\n",
    "            print(\"\\n%d: Loading snapshots for c = %f, sigma = %f\"%(ctr, c[ctr], sigma[ctr]))\n",
    "            snap_data[ctr], times_offline[ctr], nodes, Nx, Ny, \\\n",
    "                    snap_start[ctr], snap_end[ctr] = du.read_data(data, soln_names)\n",
    "            ctr +=1\n",
    "            \n",
    "        del data\n",
    "        gc.collect()\n",
    "    \n",
    "    elif param == 3:\n",
    "        for indx,val in enumerate([41,42,]):\n",
    "            datafile = 'gaussian2d_pulse_500x200_c%.2f_sigma5.0000_y0%d.npz'%(param,val)\n",
    "            data = np.load(os.path.join(data_dir, datafile))\n",
    "\n",
    "            c[ctr] = float(datafile.split('_c')[1].split('_')[0])\n",
    "            sigma[ctr] = float(datafile.split('_sigma')[1].split('_y0')[0])\n",
    "            print(\"\\n%d: Loading snapshots for c = %f, sigma = %f\"%(ctr, c[ctr], sigma[ctr]))\n",
    "            snap_data[ctr], times_offline[ctr], nodes, Nx, Ny, \\\n",
    "                    snap_start[ctr], snap_end[ctr] = du.read_data(data, soln_names)\n",
    "            ctr +=1\n",
    "            \n",
    "        del data\n",
    "        gc.collect()\n",
    "\n",
    "    elif param == 4:\n",
    "        for indx,val in enumerate([41,42,43]):\n",
    "            datafile = 'gaussian2d_pulse_500x200_c%.2f_sigma5.0000_y0%d.npz'%(param,val)\n",
    "            data = np.load(os.path.join(data_dir, datafile))\n",
    "\n",
    "            c[ctr] = float(datafile.split('_c')[1].split('_')[0])\n",
    "            sigma[ctr] = float(datafile.split('_sigma')[1].split('_y0')[0])\n",
    "            print(\"\\n%d: Loading snapshots for c = %f, sigma = %f\"%(ctr, c[ctr], sigma[ctr]))\n",
    "            snap_data[ctr], times_offline[ctr], nodes, Nx, Ny, \\\n",
    "                    snap_start[ctr], snap_end[ctr] = du.read_data(data, soln_names)\n",
    "            ctr +=1\n",
    "            \n",
    "        del data\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "if flag == 'speed':\n",
    "    param_list = list(c.values())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4f26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:30.185754Z",
     "start_time": "2022-02-15T21:47:30.180343Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Prepare training snapshots\n",
    "\n",
    "Nn = Nx * Ny\n",
    "\n",
    "## number of steps to skip in selecting training snapshots for SVD basis\n",
    "snap_incr=2\n",
    "\n",
    "## Subsample snapshots for building reduced basis\n",
    "Nt = {}\n",
    "Nt_train = {}\n",
    "for indx,val in enumerate(param_list):\n",
    "    Nt[indx] = times_offline[indx].size\n",
    "    Nt_train[indx] = times_offline[indx][:snap_end[indx]+1:snap_incr].size\n",
    "    print('{5}: Using {0} training snapshots for {1} = {2:.2f} in time interval [{3:.2f},{4:.2f}] mins'.format(\n",
    "                            Nt_train[indx], flag, param_list[indx], times_offline[indx][:snap_end[indx]+1:snap_incr][0]/60, \n",
    "                            times_offline[indx][:snap_end[indx]+1:snap_incr][-1]/60, indx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1588c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:30.189737Z",
     "start_time": "2022-02-15T21:47:30.187017Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def set_label(key):\n",
    "    if key == 'pulse':\n",
    "        ky = 'u'\n",
    "    \n",
    "    return ky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129ec6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:31.060919Z",
     "start_time": "2022-02-15T21:47:30.191198Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Load shifted snapshots   \n",
    "print(\"Loading high-fidelity shifted snapshots -----\\n\")\n",
    "\n",
    "shifted_snap = {}\n",
    "for indx,val in enumerate(param_list):\n",
    "    if flag == 'sigma':\n",
    "        shift_datafile = 'Shift_Gaussian2d_pulse_500x200_c1.00_sigma'+'%.4f'%val+'.npz'\n",
    "    elif flag == 'speed':\n",
    "        shift_datafile = 'Shift_Gaussian2d_pulse_500x200_c'+'%.2f'%val+'_sigma5.0000.npz'\n",
    "    \n",
    "    print(\"\\nReading data from %s\"%shift_datafile)\n",
    "    shift_data = np.load(os.path.join(data_dir, shift_datafile))\n",
    "    shifted_snap[indx] = {}\n",
    "    for key in soln_names:\n",
    "        shifted_snap[indx][key] = np.outer(shift_data[key].reshape((Nn,-1)),np.ones(Nt[indx]))\n",
    "\n",
    "        print(\"%d: Loading %d shifted snapshots for c = %f, sigma = %f, key = %s\"%(indx, \n",
    "                                                            shifted_snap[indx][key].shape[1], \n",
    "                                                            c[indx], sigma[indx], key))\n",
    "        \n",
    "del shift_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1446c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:45.411016Z",
     "start_time": "2022-02-15T21:47:31.062148Z"
    }
   },
   "outputs": [],
   "source": [
    "###  ***** Prepare Autoencoder input data *******\n",
    "## 1. Concatenate parametric snapshots to prepare\n",
    "##    input features for AA Autoencoder training\n",
    "\n",
    "u = np.empty((0,Nn),)\n",
    "u_shift = np.empty((0,Nn),)\n",
    "\n",
    "p_max = np.asarray(param_list).max()\n",
    "for indx,val in enumerate(param_list):\n",
    "    u_snap = snap_data[indx]['pulse'][:,:snap_end[indx]+1:snap_incr].T    \n",
    "    u = np.vstack((u,u_snap))\n",
    "    u_shift_snap = shifted_snap[indx]['pulse'][:,:snap_end[indx]+1:snap_incr].T\n",
    "    u_shift = np.vstack((u_shift,u_shift_snap))\n",
    "    \n",
    "\n",
    "validation_data = True\n",
    "skip_start = snap_incr//2\n",
    "if validation_data:\n",
    "    u_val = np.empty((0,Nn),)\n",
    "    u_val_shift = np.empty((0,Nn),)\n",
    "    for indx,val in enumerate(param_list):\n",
    "        u_val_snap = snap_data[indx]['pulse'][:,skip_start:snap_end[indx]+1:snap_incr].T\n",
    "        u_val_shift_snap = shifted_snap[indx]['pulse'][:,skip_start:snap_end[indx]+1:snap_incr].T\n",
    "        u_val = np.vstack((u_val, u_val_snap))\n",
    "        u_val_shift = np.vstack((u_val_shift, u_val_shift_snap))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d319e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:47.525225Z",
     "start_time": "2022-02-15T21:47:45.413149Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Scale Input Features to lie between [0,1]\n",
    "\n",
    "scaling=True\n",
    "\n",
    "if scaling:\n",
    "    u, u_shift, u_max, u_min = du.data_scaler(u, u_shift)\n",
    "\n",
    "    if validation_data:\n",
    "        u_val, u_val_shift, _, _ = du.data_scaler(u_val, u_val_shift, u_max, u_min)\n",
    "        \n",
    "else:\n",
    "    u_max = np.maximum(u.max(), u_shift.max()); u_min = np.minimum(u.min(), u_shift.min())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2393e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:49.278208Z",
     "start_time": "2022-02-15T21:47:47.527107Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "###  ***** Prepare Autoencoder input data *******\n",
    "## 2. Augment input feature states with scaled \n",
    "##    parameter values\n",
    "\n",
    "u_sigma = np.empty((0,1),)\n",
    "u_shift_sigma = np.empty((0,1),)\n",
    "\n",
    "p_max = np.asarray(param_list).max()\n",
    "for indx,val in enumerate(param_list/p_max):\n",
    "    u_sigma = np.vstack((u_sigma,val*np.ones((Nt_train[indx],1))))\n",
    "    u_shift_sigma = np.vstack((u_shift_sigma,val*np.ones((Nt_train[indx],1)) ))\n",
    "    \n",
    "u = np.hstack(( u,u_sigma))\n",
    "u_shift = np.hstack(( u_shift, u_shift_sigma))\n",
    "\n",
    "\n",
    "if validation_data:\n",
    "    u_val_sigma = np.empty((0,1),)\n",
    "    u_val_shift_sigma = np.empty((0,1),)\n",
    "    for indx,val in enumerate(param_list/p_max):\n",
    "        val_steps = np.minimum(times_offline[indx][skip_start:snap_end[indx]+1:snap_incr].size, Nt_train[indx])\n",
    "        u_val_sigma = np.vstack((u_val_sigma, val*np.ones((val_steps,1)) ))\n",
    "        u_val_shift_sigma = np.vstack((u_val_shift_sigma, val*np.ones((val_steps,1)) ))\n",
    "        \n",
    "        \n",
    "    u_val = np.hstack(( u_val, u_val_sigma))\n",
    "    u_val_shift = np.hstack(( u_val_shift, u_val_shift_sigma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9d576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:52.370742Z",
     "start_time": "2022-02-15T21:47:49.279586Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Define minibatch generators for training and\n",
    "## validation using Tensorflow Dataset API\n",
    "\n",
    "batch_size = 24\n",
    "\n",
    "size_buffer = max(list(Nt_train.values()))\n",
    "    \n",
    "u_train, u_val = tu.gen_batch_ae(u, u_shift, u_val, u_val_shift, \n",
    "                                 batch_size=batch_size, shuffle_buffer=size_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cfb14e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:52.377478Z",
     "start_time": "2022-02-15T21:47:52.372410Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Design of the AA autoencoder sub-networks\n",
    "\n",
    "size0 = u.shape[1]\n",
    "size1 = int(np.floor(size0//160))\n",
    "size2 = int(np.floor(size0//400))\n",
    "size3 = int(np.floor(size0//1600))\n",
    "\n",
    "size = np.array([size0, size1, size2, size3])\n",
    "latent_dim_u = 15\n",
    "print(\"Full order dimension: \",size0)\n",
    "print(\"Hidden dimensions: \", size[1:])\n",
    "print(\"Latent dimension: \",latent_dim_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b317c289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:47:52.393766Z",
     "start_time": "2022-02-15T21:47:52.380056Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "## Custom training loop for AA Autoencoder model\n",
    "\n",
    "def train_AAautoencoder(epochs, train_dataset, val_dataset, latent_dim, actvn, size, Nt, **kwargs):\n",
    "    \n",
    "    loss_wt = kwargs['loss_weights']\n",
    "    model = aa.AAautoencoder(latent_dim, actvn, size,)\n",
    "    \n",
    "            \n",
    "    try:\n",
    "        learn_rate = kwargs['lr']\n",
    "    except:\n",
    "        learn_rate = 0.0001\n",
    "    try:\n",
    "        batch_size = kwargs['batch_size']\n",
    "    except:\n",
    "        batch_size = 64\n",
    "    try:\n",
    "        reg_wt = kwargs['reg']\n",
    "    except:\n",
    "        reg_wt = 0.02\n",
    "        \n",
    "    try:\n",
    "        lb = kwargs['loss_lb']\n",
    "    except:\n",
    "        lb = 0.9\n",
    "        \n",
    "    try:\n",
    "        decay_factor = kwargs['decay'][0]; decay_rate = kwargs['decay'][1]\n",
    "    except:\n",
    "        decay_factor = 15; decay_rate = 0.9\n",
    "    \n",
    "    try: \n",
    "        learning_rate_decay = kwargs['lr_decay']\n",
    "        init_learn_rate = tf.keras.optimizers.schedules.ExponentialDecay(learn_rate, \n",
    "                                                        decay_steps=epochs*Nt//batch_size//decay_factor,\n",
    "                                                        decay_rate=decay_rate, staircase=True)\n",
    "    except:\n",
    "        init_learn_rate = learn_rate\n",
    "        \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=init_learn_rate)\n",
    "    \n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    shift_loss = []\n",
    "    recon_loss = []\n",
    "    lr = []\n",
    "    regu_loss = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        train_loss_value = 0; \n",
    "        shift_loss_value = 0; \n",
    "        recon_loss_value = 0;\n",
    "        rloss_value = 0\n",
    "        for step_train, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "            # Open a GradientTape to record the operations run\n",
    "            # during the forward pass, which enables auto-differentiation.\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # Run the forward pass of the layer.\n",
    "              \n",
    "                \n",
    "                ## Encoded output for this minibatch\n",
    "                encoded = model.encoder(x_batch_train, training=True)\n",
    "                \n",
    "                ## Shift Decoder Evaluation for this minibatch\n",
    "                pred = model.shift(encoded, training=True)  \n",
    "                \n",
    "                ## True Decoder Evaluation for this minibatch\n",
    "                recon = model.decoder(encoded, training=True)\n",
    "                \n",
    "                ## L2 Regularization Loss Component\n",
    "                l2_loss=tf.add_n(model.losses)\n",
    "                \n",
    "               \n",
    "                ## Compute the loss value for this minibatch.\n",
    "                loss1 = tu.comb_loss(y_batch_train[:,:], pred, lb=lb, delta=0.5) #\\\n",
    "                loss2 = tu.comb_loss(x_batch_train[:,:], recon, lb=lb, delta=0.5) #\\\n",
    "\n",
    "                if epoch < kwargs['segmented']:\n",
    "                    loss_value = loss_wt[0] * loss1 + loss_wt[1] * loss2 \n",
    "                else:\n",
    "                    loss_value = loss_wt[2] * loss1 + loss_wt[3] * loss2\n",
    "                \n",
    "                reg_loss = reg_wt*tf.sqrt(l2_loss)/(epoch+1)\n",
    "                loss_value += reg_loss\n",
    "                rloss_value +=reg_loss\n",
    "                       \n",
    "            train_loss_value += loss_value\n",
    "            shift_loss_value += loss1\n",
    "            recon_loss_value += loss2\n",
    "            \n",
    "            # Use the gradient tape to automatically retrieve\n",
    "            # the gradients of the trainable variables with respect to the loss.\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables,\n",
    "                        unconnected_gradients=tf.UnconnectedGradients.ZERO) \n",
    "            ## Suppress warnings about zero gradients during training\n",
    "            \n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # Log every 10 batches.\n",
    "            if step_train % 10 == 0:\n",
    "                print(\"Training loss for batch %d: Shift: %.4e, Recon: %.4e, Reg: %.4e\"\n",
    "                    % ((step_train+1), float(loss1), float(loss2), float(reg_loss)))\n",
    "                print(\"Seen so far: %s samples\" % ((step_train + 1) * batch_size))\n",
    "                \n",
    "        train_loss.append((train_loss_value/(step_train + 1)).numpy())\n",
    "        shift_loss.append((shift_loss_value/(step_train + 1)).numpy())\n",
    "        recon_loss.append((recon_loss_value/(step_train + 1)).numpy())\n",
    "        regu_loss.append((rloss_value/(step_train + 1)).numpy())\n",
    "        \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        val_loss_value = 0; \n",
    "        for step_val, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "            \n",
    "            encoded_val = model.encoder(x_batch_val, training=False)\n",
    "            pred_val = model.shift(encoded_val, training=False)\n",
    "            recon_val = model.decoder(encoded_val, training=False)\n",
    "            \n",
    "            loss_val1 = tu.comb_loss(y_batch_val[:,:], pred_val, lb=lb,delta=0.5) #\\\n",
    "            loss_val2 = tu.comb_loss(x_batch_val[:,:], recon_val, lb=lb,delta=0.5) #\\\n",
    "            \n",
    "            if epoch < kwargs['segmented']:\n",
    "                val_loss_value += loss_wt[0] * loss_val1 + loss_wt[1] * loss_val2 \n",
    "            else:\n",
    "                val_loss_value += loss_wt[2] * loss_val1 + loss_wt[3] * loss_val2\n",
    "        \n",
    "        val_loss_value += rloss_value\n",
    "        val_loss.append((val_loss_value/(step_val+1)).numpy())\n",
    "        \n",
    "        if learning_rate_decay:\n",
    "            print(\"Epoch %d, Training Loss: %.4e, Validation Loss: %.4e. LR: %.4e\" \n",
    "                  % (epoch, float(train_loss[-1]),float(val_loss[-1]),init_learn_rate(optimizer.iterations).numpy()))\n",
    "            lr.append(init_learn_rate(optimizer.iterations).numpy())\n",
    "        else:\n",
    "            print(\"Epoch %d, Training Loss: %.4e, Validation Loss: %.4e. LR: %.4e\" \n",
    "                  % (epoch, float(train_loss[-1]),float(val_loss[-1]),learn_rate))\n",
    "            lr.append(learn_rate)\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"****Total training time = {0}****\\n\".format(end_time - start_time))\n",
    "    \n",
    "    return model, train_loss, val_loss, shift_loss, recon_loss, lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ffcb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T22:52:11.153201Z",
     "start_time": "2022-02-15T22:52:10.408247Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Load AE model\n",
    "\n",
    "load_ae_model = False\n",
    "\n",
    "if load_ae_model:\n",
    "#     pre_trained_dir = '/gpfs/cwfs/sdutta/ae_data/pulse/ae_shift_inv_comb_models/saved_model_AA4'\n",
    "    pre_trained_dir = model_dir+'ae_models/saved_model_AA4'\n",
    "    mnum = pre_trained_dir.split('saved_model_')[1]\n",
    "\n",
    "    ## When using custom loss functions while training, there are two ways\n",
    "    ## to load a saved model\n",
    "    ## 1) If loaded model will not be used for retraining, then\n",
    "    ##   use 'compile=False' option while loading so that TF does\n",
    "    ##   search for loss functions\n",
    "    \n",
    "    u_autoencoder, model_training = aa.load_model(pre_trained_dir,mnum)\n",
    "    \n",
    "    print(model_training['msg'])\n",
    "\n",
    "    loss_u = model_training['loss']\n",
    "    vloss_u = model_training['valloss'] \n",
    "    sloss_u = model_training['shiftloss'] \n",
    "    rloss_u = model_training['reconloss'] \n",
    "    lr_u = model_training['lr']\n",
    "    epochs_u = model_training['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e559a25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T22:11:51.603405Z",
     "start_time": "2022-02-15T21:47:52.399879Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training the AA Autoencoder Model\n",
    "actvn = tf.keras.activations.swish \n",
    "\n",
    "if not load_ae_model:\n",
    "    if flag == 'speed':\n",
    "        u_autoencoder, loss_u, vloss_u, sloss_u, rloss_u, lr_u = train_AAautoencoder(500, u_train, u_val, \n",
    "                                                  latent_dim_u, actvn, np.array([size[0], size[2], size[3]]),\n",
    "                                                  sum(list(Nt_train.values())),\n",
    "                                                  batch_size = batch_size, loss_lb = 0.9,\n",
    "                                                  segmented = 2500, decay = [15, 0.9],\n",
    "                                                  loss_weights = [0.1, 0.88, 0.49, 0.49],\n",
    "                                                  lr = 1e-4, lr_decay = True, reg = 0.02 )\n",
    "\n",
    "    if flag == 'sigma':\n",
    "        u_autoencoder, loss_u, vloss_u, sloss_u, rloss_u, lr_u = train_AAautoencoder(epochs_u, u_train, u_val, \n",
    "                                                              latent_dim_u, actvn, size,  \n",
    "                                                              sum(list(Nt_train.values())),\n",
    "                                                              batch_size = batch_size, loss_lb = 0.9,\n",
    "                                                              segmented = 2500, decay = [17, 0.85],\n",
    "                                                              loss_weights = [0.0, 0.98, 0.1, 0.88],\n",
    "                                                              lr = 5e-4, lr_decay = True, reg = 0.02 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063607f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T22:52:20.892372Z",
     "start_time": "2022-02-15T22:52:20.838195Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "u_autoencoder.build(u.shape)\n",
    "u_autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf566a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T22:52:35.117759Z",
     "start_time": "2022-02-15T22:52:30.483937Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_u = u_autoencoder.encoder(u).numpy()\n",
    "shift_u = u_autoencoder.shift(encoded_u).numpy()\n",
    "decoded_u = u_autoencoder.decoder(encoded_u).numpy()\n",
    "\n",
    "encoded = encoded_u\n",
    "\n",
    "\n",
    "print('\\n*********AE shifted decoder reconstruction error*********\\n')\n",
    "print('u  Shift MSE: ' + str(np.mean(np.square(du.scaler_inverse(shift_u[:,:-1], u_max, u_min, scaling=scaling)-du.scaler_inverse(u_shift[:,:-1], u_max, u_min, scaling=scaling)))))\n",
    "\n",
    "print('\\n*********AE inverse decoder reconstruction error*********\\n')\n",
    "print('u  Reconstruction MSE: ' + str(np.mean(np.square(du.scaler_inverse(decoded_u[:,:-1], u_max, u_min, scaling=scaling)-du.scaler_inverse(u[:,:-1], u_max, u_min, scaling=scaling)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe6c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T22:51:37.869643Z",
     "start_time": "2022-02-15T22:51:31.168478Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Save the trained AE model\n",
    "\n",
    "\n",
    "    \n",
    "if not load_ae_model:\n",
    "    \n",
    "    !mkdir -p /gpfs/cwfs/sdutta/ae_data/pulse/ae_shift_inv_comb_models/saved_model_AA4\n",
    "    if flag == 'sigma':\n",
    "#         savedir = '/gpfs/cwfs/sdutta/ae_data/pulse/ae_shift_inv_comb_models/saved_model_AA1'\n",
    "        savedir = model_dir+'ae_models/saved_model_AA1'\n",
    "    elif flag == 'speed':\n",
    "#         savedir = '/gpfs/cwfs/sdutta/ae_data/pulse/ae_shift_inv_comb_models/saved_model_AA4'\n",
    "        savedir = model_dir+'ae_models/saved_model_AA4'\n",
    "\n",
    "    mnum = savedir.split('saved_model_')[1]\n",
    "    \n",
    "    if flag == 'sigma':\n",
    "        msg = 'Sigma = [5,10,20], speed = [1],'\\\n",
    "        +'Trains both shift and true decoder,'\\\n",
    "        +'\\nStep decay LR scheduler starting from 5e-4, Batch Size = 24,'\\\n",
    "        +'\\nDecaying 15% every 96 epochs. Trained for 6000 epochs,'\\\n",
    "        +'\\nFor epochs <=2500: Loss = 0.00*Shift + 0.98*True,'\\\n",
    "        +'\\nFor epochs > 2500: Loss = 0.10*Shift + 0.88*True,'\\\n",
    "        +'\\n90% NMSE + 10% Huber, L2 regularization, Scaling to [0,1],'\\\n",
    "        +'\\nBoth Encoder input & Decoder ouput are augmented by parameter value'\n",
    "    elif flag == 'speed':\n",
    "        msg = 'Sigma = [5], speed = [1,2,3,4],'\\\n",
    "        +'Trains both shift and true decoder,'\\\n",
    "        +'\\nStep decay LR scheduler starting from 1e-4, Batch Size = 24,'\\\n",
    "        +'\\nDecaying 10% every 390 epochs. Trained for 6000 epochs,'\\\n",
    "        +'\\nFor epochs <=2500: Loss = 0.10*Shift + 0.88*True,'\\\n",
    "        +'\\nFor epochs > 2500: Loss = 0.49*Shift + 0.49*True,'\\\n",
    "        +'\\n90% NMSE + 10% Huber, L2 regularization, Scaling to [0,1],'\\\n",
    "        +'\\nBoth Encoder input & Decoder ouput are augmented by parameter value'\n",
    "    print(\"\\n===========\")\n",
    "    print(msg)\n",
    "    \n",
    "    \n",
    "    model_results = {'loss': loss_u, 'valloss': vloss_u,\n",
    "                     'shiftloss': sloss_u, 'reconloss': rloss_u,\n",
    "                     'lr': lr_u, 'epochs': epochs_u, 'msg': msg,\n",
    "                     'savedir': savedir}\n",
    "    \n",
    "    aa.save_model(u_autoencoder, u, mnum, model_results)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b151d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:06.946050Z",
     "start_time": "2022-02-15T21:44:06.399280Z"
    },
    "code_folding": [
     2
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "param_num = 0\n",
    "\n",
    "def select_variable(param_num):\n",
    "    p_indx = sum(list(Nt_train.values())[:param_num]); \n",
    "    encoded = encoded_u[p_indx:p_indx+Nt_train[param_num]]; ldim = latent_dim_u\n",
    "    return encoded, ldim\n",
    "\n",
    "def plot_latent_param_nums(param_num):\n",
    "\n",
    "    num_rows = 5 #np.maximum(4, encoded.shape[1]//4)\n",
    "    fig, ax = plt.subplots(nrows=num_rows,ncols=1,figsize=(6,num_rows*1.75),constrained_layout=True)    \n",
    "\n",
    "    for ix,comp in enumerate([param_num]):\n",
    "        encoded, ldim = select_variable(comp)\n",
    "        \n",
    "        for i in range(num_rows):\n",
    "            tt = ax[i].plot(times_offline[comp][:snap_end[comp]+1:snap_incr],encoded[:,i],label='mode %d'%i,marker='o',markevery=20)\n",
    "            ax[i].legend()\n",
    "        ax[i].set_xlabel('Time')  \n",
    "        fig.suptitle('AE modes for %s = %.2f'%(flag, param_list[param_num]),fontsize=16)  \n",
    "\n",
    "\n",
    "plot_latent_param_nums(param_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7c88f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:07.786782Z",
     "start_time": "2022-02-15T21:44:06.947240Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Visualization losses and learning rate evolution\n",
    "num_epochs_u = np.arange(epochs_u)\n",
    "\n",
    "pu.plot_training(num_epochs_u, loss_u, vloss_u, lr_u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05838696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:30.146534Z",
     "start_time": "2022-02-15T21:44:29.747727Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(8,4),constrained_layout=True)\n",
    "ax.semilogy(num_epochs_u, sloss_u,'b-.',label='u:shift_loss')\n",
    "ax.semilogy(num_epochs_u, rloss_u,'k--',label='u:recon_loss')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52977c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:45.190261Z",
     "start_time": "2022-02-15T21:44:42.602187Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Predict using all high-fidelity data points \n",
    "### for parameter values in the training set\n",
    "\n",
    "iparam = np.random.choice(len(param_list))\n",
    "print(\"Testing AE predictions for %s = %f (parameter number = %d)\"%(flag,param_list[iparam], iparam))\n",
    "val = param_list[iparam]/p_max\n",
    "\n",
    "u_test = np.hstack(( snap_data[iparam]['pulse'][:,:snap_end[iparam]+1].T, val*np.ones((Nt[iparam],1)) ))\n",
    "u_shift_test = np.hstack(( shifted_snap[iparam]['pulse'][:,:snap_end[iparam]+1].T, val*np.ones((Nt[iparam],1)) ))\n",
    "\n",
    "if scaling:\n",
    "    u_test, u_shift_test, _, _ = du.data_scaler(u_test, u_shift_test, u_max, u_min)\n",
    "\n",
    "\n",
    "encoded_u_test = u_autoencoder.encoder(u_test).numpy()\n",
    "shift_u_test = u_autoencoder.shift(encoded_u_test).numpy()\n",
    "recon_u_test = u_autoencoder.decoder(encoded_u_test).numpy()\n",
    "\n",
    "pred_shift = {}\n",
    "pred_shift['pulse'] = du.scaler_inverse(shift_u_test[:,:-1], u_max, u_min, scaling=scaling).T\n",
    "\n",
    "pred_recon = {}\n",
    "pred_recon['pulse'] = du.scaler_inverse(recon_u_test[:,:-1], u_max, u_min, scaling=scaling).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418ffc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:45.566874Z",
     "start_time": "2022-02-15T21:44:45.191976Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Visualizing reconstruction of shifted snapshots using shift decoder  \n",
    "    \n",
    "fig = plt.figure(figsize=(14,8))\n",
    "ky = 'pulse'; \n",
    "np.random.seed(2021)\n",
    "iplot = 80 \n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.82)\n",
    "\n",
    "pu.compare_soln(pred_shift,shifted_snap[iparam],iplot,times_offline[iparam],times_offline[iparam],Nx,Ny,ky,flag='AE')\n",
    "fig.suptitle(\"Comparing AE predictions of shifted snapshots \\nfor %s = %d at t = %.2f mins\"%(flag, param_list[iparam], times_offline[iparam][iplot]/60),fontsize=18, y=0.98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866ea17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:45.928118Z",
     "start_time": "2022-02-15T21:44:45.568568Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Visualizing direct reconstruction using true decoder\n",
    "\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.82)\n",
    "\n",
    "pu.compare_soln(pred_recon,snap_data[iparam],iplot,times_offline[iparam],times_offline[iparam],Nx,Ny,ky,flag='AE')\n",
    "fig.suptitle(\"Comparing AE predictions of true snapshots \\nfor %s = %d at t = %.2f mins\"%(flag, param_list[iparam], times_offline[iparam][iplot]/60),fontsize=18, y=0.98)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb01b1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:47.412675Z",
     "start_time": "2022-02-15T21:44:45.929578Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Compute rel error for decoded prediction of shifted and true solutions\n",
    "rel_err_shift = {}\n",
    "rel_err_recon = {}\n",
    "\n",
    "for key in soln_names:\n",
    "    true = shifted_snap[iparam][key][:,:snap_end[iparam]+1]\n",
    "    true2 = snap_data[iparam][key][:,:snap_end[iparam]+1]\n",
    "    rel_err_shift[key] = np.linalg.norm(pred_shift[key][:,:]-true,axis=0)/np.linalg.norm(true,axis=0)\n",
    "    rel_err_recon[key] = np.linalg.norm(pred_recon[key][:,:]-true2,axis=0)/np.linalg.norm(true2,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d14507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:47.734492Z",
     "start_time": "2022-02-15T21:44:47.413991Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "### Visualize relative errors for both decoders\n",
    "\n",
    "fig = plt.figure(figsize=(16,5))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "indx = times_offline[iparam][:snap_end[iparam]+1]/60\n",
    "val_mark = int(times_offline[iparam][:snap_end[iparam]+1].size*1.0)-30\n",
    "    \n",
    "if validation_data:\n",
    "    val_skip = 2\n",
    "else:\n",
    "    val_skip = 1\n",
    "\n",
    "val_dict = {'v_data': validation_data, 'v_mark': val_mark, 'v_skip': val_skip, 'legend': True}\n",
    "pu.plot_rel_err(indx,rel_err_shift,rel_err_recon,val_dict, ky='pulse')\n",
    "fig.suptitle('Relative errors of AE prediction for speed = %d'%(c[iparam]),fontsize=18, y=0.98)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6dec26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:48.839236Z",
     "start_time": "2022-02-15T21:44:47.735691Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Evaluate predictions on a dataset with unseen parameter value\n",
    "\n",
    "new_test = True\n",
    "\n",
    "if new_test:\n",
    "    if flag == 'speed':\n",
    "        test_param = np.random.choice([5,6,8])\n",
    "        c_new = test_param; sigma_new = 8\n",
    "        new_data = np.load(os.path.join(data_dir,'Gaussian2d_pulse_500x200_c%.2f_sigma5.0000.npz'%c_new))\n",
    "        \n",
    "    elif flag == 'sigma':\n",
    "        test_param = np.random.choice([ 8, 16])  #3.3333, 4,\n",
    "        c_new = 1; sigma_new = test_param\n",
    "        new_data = np.load(os.path.join(data_dir,'Gaussian2d_pulse_500x200_c1.00_sigma%.4f.npz'%sigma_new))\n",
    "        \n",
    "    \n",
    "    print(\"Chosen parameter: %s = %f\"%(flag, test_param))\n",
    "    print(\"Loading pre-computed true snapshots -----\\n\")\n",
    "    \n",
    "    snap_new, times_new, nodes2, Nx2, Ny2, snap_start_new, snap_end_new = du.read_data(new_data, soln_names)\n",
    "    Nt_new = times_new.size\n",
    "\n",
    "    \n",
    "    if flag == 'speed':\n",
    "        new_shift_data = np.load(os.path.join(data_dir,'Shift_Gaussian2d_pulse_500x200_c%.2f_sigma5.0000.npz'%c_new))\n",
    "    elif flag == 'sigma':\n",
    "        new_shift_data = np.load(os.path.join(data_dir,'Shift_Gaussian2d_pulse_500x200_c1.00_sigma%.4f.npz'%sigma_new))\n",
    "    \n",
    "    print(\"Loading pre-computed shifted snapshots -----\\n\")\n",
    "\n",
    "    shifted_snap_new = {}\n",
    "    for key in soln_names:\n",
    "        tmp = new_shift_data[key].reshape((Nn,-1))\n",
    "        shifted_snap_new[key] = np.outer(tmp,np.ones(Nt_new))\n",
    "\n",
    "    del new_data\n",
    "    del new_shift_data\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "    u_new = snap_new['pulse'][:,:snap_end_new+1].T\n",
    "    u_shift_new = shifted_snap_new['pulse'][:,:snap_end_new+1].T\n",
    "    \n",
    "    u_new = np.hstack(( u_new, (test_param/p_max)*np.ones((Nt_new,1)) ))\n",
    "    u_shift_new = np.hstack(( u_shift_new, (test_param/p_max)*np.ones((Nt_new,1)) ))\n",
    "\n",
    "    if scaling:\n",
    "        u_new, u_shift_new, _, _ = du.data_scaler(u_new, u_shift_new, u_max, u_min)\n",
    "\n",
    "    encoded_u_new = u_autoencoder.encoder(u_new).numpy()\n",
    "    shift_u_new = u_autoencoder.shift(encoded_u_new).numpy()\n",
    "    recon_u_new = u_autoencoder.decoder(encoded_u_new).numpy()\n",
    "\n",
    "\n",
    "    pred_shift_new = {}\n",
    "    pred_shift_new['pulse'] = du.scaler_inverse(shift_u_new[:,:-1], u_max, u_min, scaling=scaling).T\n",
    "\n",
    "    pred_recon_new = {}\n",
    "    pred_recon_new['pulse'] = du.scaler_inverse(recon_u_new[:,:-1], u_max, u_min, scaling=scaling).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25d512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:49.207181Z",
     "start_time": "2022-02-15T21:44:48.840807Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Visualizing reconstruction of shifted snapshots using shift decoder\n",
    "if new_test:\n",
    "    fig = plt.figure(figsize=(14,8))\n",
    "    ky = 'pulse'; iplot = 40 #np.random.choice(times_new.size)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.82)\n",
    "    pu.compare_soln(pred_shift_new,shifted_snap_new,iplot,times_new,times_new,Nx,Ny,ky,flag='AE')\n",
    "    fig.suptitle(\"Comparing AE predictions of shifted snapshots \\nfor %s = %.2f at t = %.2f mins\"%(flag, test_param, times_new[iplot]/60),fontsize=18, y=0.98)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05632c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:49.566544Z",
     "start_time": "2022-02-15T21:44:49.208437Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Visualizing direct reconstruction using true decoder\n",
    "if new_test:\n",
    "    fig = plt.figure(figsize=(14,8))\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.82)\n",
    "    pu.compare_soln(pred_recon_new,snap_new,iplot,times_new,times_new,Nx,Ny,ky,flag='AE')\n",
    "    fig.suptitle(\"Comparing AE predictions of true snapshots \\nfor %s = %.2f at t = %.2f mins\"%(flag, test_param, times_new[iplot]/60),fontsize=18, y=0.98)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0719b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:49.870507Z",
     "start_time": "2022-02-15T21:44:49.568261Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if new_test:\n",
    "    ## Compute rel error for decoded prediction of shifted solutions\n",
    "    rel_err_shift_new = {}\n",
    "    rel_err_recon_new = {}\n",
    "\n",
    "    for key in soln_names:\n",
    "        true_new = shifted_snap_new[key][:,:snap_end_new+1]\n",
    "        true2_new = snap_new[key][:,:snap_end_new+1]\n",
    "        rel_err_shift_new[key] = np.linalg.norm(pred_shift_new[key][:,:]-true_new,axis=0)/np.linalg.norm(true_new,axis=0)\n",
    "        rel_err_recon_new[key] = np.linalg.norm(pred_recon_new[key][:,:]-true2_new,axis=0)/np.linalg.norm(true2_new,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcd2ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T21:44:50.098459Z",
     "start_time": "2022-02-15T21:44:49.871884Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if new_test:\n",
    "    ### Visualize relative errors for both decoders\n",
    "\n",
    "    fig = plt.figure(figsize=(16,5))\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    indx = times_new[:snap_end_new+1]/60\n",
    "\n",
    "    val_dict_new = {'v_data': False, 'v_mark': indx.size, 'v_skip': 1, 'legend': False}\n",
    "    pu.plot_rel_err(indx, rel_err_shift_new, rel_err_recon_new, val_dict_new, ky='pulse')\n",
    "    fig.suptitle('Relative errors of AE prediction for speed = %d'%(c_new),fontsize=18, y=0.98)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596a0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
